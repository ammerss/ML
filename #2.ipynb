{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "\n",
    "transform = transforms.Compose([#transforms.Resize((256,256)),  \n",
    "                                transforms.Grayscale(),\t\t# the code transforms.Graysclae() is for changing the size [3,100,100] to [1, 100, 100] (notice : [channel, height, width] )\n",
    "                                transforms.ToTensor(),])\n",
    "\n",
    "\n",
    "#train_data_path = 'relative path of training data set'\n",
    "train_data_path = 'horse-or-human/horse-or-human/train'\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "# if shuffle=True, the data reshuffled at every epoch #numworks가 뭐지?\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1027, shuffle=False, num_workers=0)  \n",
    "\n",
    "\n",
    "validation_data_path = 'horse-or-human/horse-or-human/validation'\n",
    "valset = torchvision.datasets.ImageFolder(root=validation_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=256, shuffle=False, num_workers=0)  \n",
    "\n",
    "NUM_EPOCH=1\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    # load training images of the batch size for every iteration\n",
    "    for i, data in enumerate(trainloader):\n",
    "\n",
    "        # inputs is the image\n",
    "        # labels is the class of the image\n",
    "        inputs, labels = data\n",
    "\n",
    "        # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "        #print(labels)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # load validation images of the batch size for every iteration\n",
    "    for i, data in enumerate(valloader):\n",
    "        \n",
    "        # inputs is the image\n",
    "        # labels is the class of the image\n",
    "        inputs, labels = data\n",
    "\n",
    "        # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "        #print(labels)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = next(iter(trainloader))[0].numpy()\n",
    "train_label =next(iter(trainloader))[1].numpy()\n",
    "\n",
    "val_dataset = next(iter(valloader))[0].numpy()\n",
    "val_label =next(iter(valloader))[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=train_label.reshape(1027,1)\n",
    "val_label=val_label.reshape(256,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flatten=train_dataset.reshape(1027,train_dataset.shape[2]*train_dataset.shape[3])\n",
    "val_flatten=val_dataset.reshape(256,val_dataset.shape[2]*val_dataset.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1027)\n",
      "(1, 1027)\n",
      "(10000, 256)\n",
      "(1, 256)\n"
     ]
    }
   ],
   "source": [
    "x_train=train_flatten.T\n",
    "y_train=train_label.T\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "x_test=val_flatten.T\n",
    "y_test=val_label.T\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights_and_bias(dimension):\n",
    "    w = np.full((dimension,1),0.01)\n",
    "    b = 0.0\n",
    "    return w, b\n",
    "\n",
    "def sigmoid(z):\n",
    "    y_head = 1/(1+np.exp(-z))\n",
    "    return y_head\n",
    "\n",
    "def forward_backward_propagation(w,b,x_train,y_train):\n",
    "    delta=1e-7\n",
    "    # forward propagation\n",
    "    z = np.dot(w.T,x_train) + b\n",
    "    y_head = sigmoid(z)\n",
    "    loss = -y_train*np.log(y_head+delta)-(1-y_train)*np.log((1-y_head)+delta)\n",
    "    cost = (np.sum(loss))/x_train.shape[1]\n",
    "    # backward propagation\n",
    "    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1]\n",
    "    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n",
    "    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n",
    "    return cost,gradients\n",
    "\n",
    "def loss_fnc(w,b,x_test,y_test):\n",
    "    delta=1e-7\n",
    "    z = np.dot(w.T,x_test) + b\n",
    "    y_head = sigmoid(z)\n",
    "    loss = -y_test*np.log(y_head+delta)-(1-y_test)*np.log((1-y_head)+delta)\n",
    "    cost = (np.sum(loss))/x_test.shape[1]\n",
    "    return cost\n",
    "\n",
    "def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n",
    "    cost_list = []\n",
    "    cost_list2 = []\n",
    "    index = []\n",
    "    val_list = []\n",
    "    ac_train=[]\n",
    "    ac_val=[]\n",
    "    \n",
    "    for i in range(number_of_iterarion):\n",
    "        \n",
    "        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n",
    "        cost_list.append(cost)\n",
    "        \n",
    "        w = w - learning_rate * gradients[\"derivative_weight\"]\n",
    "        b = b - learning_rate * gradients[\"derivative_bias\"]\n",
    "        if i % 100 == 0:\n",
    "            cost_list2.append(cost)\n",
    "            index.append(i)\n",
    "            val_list.append(loss_fnc(w,b,x_test,y_test))\n",
    "           # print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            print(\"Iteration %i\\t Training Loss : %f\\t Validation Loss %f\"%(i,cost,loss_fnc(w,b,x_test,y_test)))\n",
    "            parameters = {\"weight\": w,\"bias\": b}\n",
    "            y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n",
    "            y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n",
    "            print(\"\\t\\t Train Accuracy: {} \\t\\t Test Accuracy: {} \".format(round(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100,2),round(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100,2)))\n",
    "            ac_train.append((round(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100,2)))\n",
    "            ac_val.append(round(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100,2))\n",
    "    \n",
    "                            \n",
    "    parameters = {\"weight\": w,\"bias\": b}\n",
    "    plt.plot(index,cost_list2)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Loss of Training Set\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(index,val_list)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Loss of Validation Set\")\n",
    "    plt.show()\n",
    "                          \n",
    "    plt.plot(index,ac_train)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Accuracy of Training Set\")\n",
    "    plt.show()\n",
    "                            \n",
    "    plt.plot(index,ac_val)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Accuracy of Validation Set\")\n",
    "    plt.show()\n",
    "                            \n",
    "    return parameters, gradients, cost_list, val_list\n",
    "\n",
    "def predict(w,b,x_test):\n",
    "    \n",
    "    z = sigmoid(np.dot(w.T,x_test)+b)\n",
    "    Y_prediction = np.zeros((1,x_test.shape[1]))\n",
    "\n",
    "    for i in range(z.shape[1]):\n",
    "        if z[0,i]<= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "\n",
    "    return Y_prediction\n",
    "\n",
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n",
    "\n",
    "    dimension =  x_train.shape[0]\n",
    "    w,b = initialize_weights_and_bias(dimension)\n",
    "\n",
    "    parameters, gradients, cost_list, val_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n",
    "    \n",
    "    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n",
    "    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n",
    "    \n",
    "    print(\"Final Results :\")\n",
    "    #print(\"\\tTrain Accuracy: {} %\".format(round(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100,2)))\n",
    "    #print(\"\\tTrain Loss: %.4f \" %(cost_list[-1]))\n",
    "    #print(\"\\tValidation Accuracy: {}%\".format(round(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100,2)))\n",
    "    #print(\"\\tValidation Loss: %.4f\"%(val_list[-1]))\n",
    "    \n",
    "    \n",
    "\n",
    "    headerColor = 'grey'\n",
    "    rowEvenColor = 'lightgrey'\n",
    "    rowOddColor = 'white'\n",
    "\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "    header=dict(\n",
    "    values=['<b>dataset</b>','<b>loss</b>','<b>accuracy</b>'],\n",
    "    line_color='darkslategray',\n",
    "    fill_color=headerColor,\n",
    "    align=['left','center'],\n",
    "    font=dict(color='white', size=12)),\n",
    "    cells=dict(\n",
    "    values=[\n",
    "      ['training', 'validation'],\n",
    "      [\"{0:.6f}\".format(cost_list[-1]), \"{0:.6f}\".format(val_list[-1])],\n",
    "      [(round(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100,2)), (round(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100,2))]],\n",
    "    line_color='darkslategray',\n",
    "    # 2-D list of colors for alternating rows\n",
    "    align = ['left', 'center'],\n",
    "    font = dict(color = 'darkslategray', size = 11)\n",
    "    ))])\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\t Training Loss : 7.845892\t Validation Loss 8.059048\n",
      "\t\t Train Accuracy: 51.31 \t\t Test Accuracy: 50.0 \n",
      "Iteration 100\t Training Loss : 1.699862\t Validation Loss 1.958832\n",
      "\t\t Train Accuracy: 58.33 \t\t Test Accuracy: 57.42 \n",
      "Iteration 200\t Training Loss : 0.925984\t Validation Loss 1.178888\n",
      "\t\t Train Accuracy: 67.87 \t\t Test Accuracy: 64.84 \n",
      "Iteration 300\t Training Loss : 0.703938\t Validation Loss 1.043585\n",
      "\t\t Train Accuracy: 74.2 \t\t Test Accuracy: 69.14 \n",
      "Iteration 400\t Training Loss : 0.575685\t Validation Loss 0.986026\n",
      "\t\t Train Accuracy: 78.97 \t\t Test Accuracy: 70.31 \n",
      "Iteration 500\t Training Loss : 0.508078\t Validation Loss 0.970229\n",
      "\t\t Train Accuracy: 80.62 \t\t Test Accuracy: 70.7 \n",
      "Iteration 600\t Training Loss : 0.260244\t Validation Loss 0.477116\n",
      "\t\t Train Accuracy: 88.7 \t\t Test Accuracy: 82.03 \n",
      "Iteration 700\t Training Loss : 0.246325\t Validation Loss 0.520084\n",
      "\t\t Train Accuracy: 89.29 \t\t Test Accuracy: 80.47 \n",
      "Iteration 800\t Training Loss : 0.254306\t Validation Loss 0.623490\n",
      "\t\t Train Accuracy: 89.29 \t\t Test Accuracy: 77.34 \n",
      "Iteration 900\t Training Loss : 0.854488\t Validation Loss 1.697109\n",
      "\t\t Train Accuracy: 72.83 \t\t Test Accuracy: 61.72 \n",
      "Iteration 1000\t Training Loss : 0.198578\t Validation Loss 0.552124\n",
      "\t\t Train Accuracy: 92.31 \t\t Test Accuracy: 79.3 \n",
      "Iteration 1100\t Training Loss : 0.187366\t Validation Loss 0.509563\n",
      "\t\t Train Accuracy: 93.18 \t\t Test Accuracy: 79.3 \n",
      "Iteration 1200\t Training Loss : 0.201264\t Validation Loss 0.701603\n",
      "\t\t Train Accuracy: 92.5 \t\t Test Accuracy: 74.61 \n",
      "Iteration 1300\t Training Loss : 0.170974\t Validation Loss 0.516163\n",
      "\t\t Train Accuracy: 94.55 \t\t Test Accuracy: 78.91 \n",
      "Iteration 1400\t Training Loss : 0.163550\t Validation Loss 0.525360\n",
      "\t\t Train Accuracy: 95.33 \t\t Test Accuracy: 78.91 \n",
      "Iteration 1500\t Training Loss : 0.158234\t Validation Loss 0.550781\n",
      "\t\t Train Accuracy: 96.01 \t\t Test Accuracy: 77.73 \n",
      "Iteration 1600\t Training Loss : 0.202873\t Validation Loss 0.825230\n",
      "\t\t Train Accuracy: 92.21 \t\t Test Accuracy: 73.83 \n",
      "Iteration 1700\t Training Loss : 0.148340\t Validation Loss 0.549407\n",
      "\t\t Train Accuracy: 96.69 \t\t Test Accuracy: 78.12 \n",
      "Iteration 1800\t Training Loss : 0.143733\t Validation Loss 0.540551\n",
      "\t\t Train Accuracy: 96.79 \t\t Test Accuracy: 77.34 \n",
      "Iteration 1900\t Training Loss : 0.139773\t Validation Loss 0.544035\n",
      "\t\t Train Accuracy: 96.98 \t\t Test Accuracy: 77.34 \n"
     ]
    }
   ],
   "source": [
    "logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
