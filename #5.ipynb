{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "\n",
    "transform = transforms.Compose([#transforms.Resize((256,256)),  \n",
    "                                transforms.Grayscale(),\t\t# the code transforms.Graysclae() is for changing the size [3,100,100] to [1, 100, 100] (notice : [channel, height, width] )\n",
    "                                transforms.ToTensor(),])\n",
    "\n",
    "\n",
    "#train_data_path = 'relative path of training data set'\n",
    "train_data_path = 'horse-or-human/horse-or-human/train'\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "# if shuffle=True, the data reshuffled at every epoch #numworks가 뭐지?\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1027, shuffle=False, num_workers=0)  \n",
    "\n",
    "\n",
    "validation_data_path = 'horse-or-human/horse-or-human/validation'\n",
    "valset = torchvision.datasets.ImageFolder(root=validation_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=256, shuffle=False, num_workers=0)  \n",
    "\n",
    "NUM_EPOCH=1\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    # load training images of the batch size for every iteration\n",
    "    for i, data in enumerate(trainloader):\n",
    "\n",
    "        # inputs is the image\n",
    "        # labels is the class of the image\n",
    "        inputs, labels = data\n",
    "\n",
    "        # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "        #print(labels)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # load validation images of the batch size for every iteration\n",
    "    for i, data in enumerate(valloader):\n",
    "        \n",
    "        # inputs is the image\n",
    "        # labels is the class of the image\n",
    "        inputs, labels = data\n",
    "\n",
    "        # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "        #print(labels)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = next(iter(trainloader))[0].numpy()\n",
    "train_label =next(iter(trainloader))[1].numpy()\n",
    "\n",
    "val_dataset = next(iter(valloader))[0].numpy()\n",
    "val_label =next(iter(valloader))[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=train_label.reshape(1027,1)\n",
    "val_label=val_label.reshape(256,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flatten=train_dataset.reshape(1027,train_dataset.shape[2]*train_dataset.shape[3])\n",
    "val_flatten=val_dataset.reshape(256,val_dataset.shape[2]*val_dataset.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train_flatten.T\n",
    "y_train=train_label.T\n",
    "\n",
    "x_test=val_flatten.T\n",
    "y_test=val_label.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = x_train.shape[0] # size of input layer`\n",
    "#n_h=10 # size of hidden layer\n",
    "n_y = y_train.shape[0] # size of output layer\n",
    "m = x_train.shape[1]\n",
    "a =0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_h = 200# hidden layer size\n",
    "#learning_rate=0.01\n",
    "\n",
    "W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "b1 = np.zeros(shape=(n_h, 1))\n",
    "\n",
    "W2 = np.random.randn(n_h,n_h) * 0.01\n",
    "b2 = np.zeros(shape=(n_h, 1))\n",
    "\n",
    "W3 = np.random.randn(n_y,n_h) * 0.01\n",
    "b3 = np.zeros(shape=(n_y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate variables\n",
    "def init_var() : \n",
    "    global W1,b1,W2,b2,W3,b3\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "    b1 = np.zeros(shape=(n_h, 1))\n",
    "\n",
    "    W2 = np.random.randn(n_h,n_h) * 0.01\n",
    "    b2 = np.zeros(shape=(n_h, 1))\n",
    "\n",
    "    W3 = np.random.randn(n_y,n_h) * 0.01\n",
    "    b3 = np.zeros(shape=(n_y, 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    y = 1/(1+np.exp(-z))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_p(x_train):\n",
    "    #print(parameters)\n",
    "    Z1 = np.dot(W1,x_train) + b1\n",
    "    A1 = leakyrelu(Z1) # activation function\n",
    "    \n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = leakyrelu(Z2)  \n",
    "    \n",
    "    Z3 = np.dot(W3,A2) + b3\n",
    "    A3 = sigmoid(Z3) # Final output prediction\n",
    "    return A3, A2 , A1, Z2, Z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cross-entropy cost\n",
    "def compute_cost(A3, Y):\n",
    "    loss = np.multiply(np.log(A3), Y) + np.multiply((1 - Y), np.log(1 - A3))\n",
    "    cost = - np.sum(loss) / m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakyrelu(z):\n",
    "    return np.where(z > 0, z, z * a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def der_leakyrelu(z):\n",
    "    return np.where(z <= 0, 0, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_p( X, Y, A3, A2, A1, Z2 ,Z1) :\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = (1 / m) * np.dot(dZ3, A2.T)\n",
    "    db3 = (1 / m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    dZ2 = np.multiply(np.dot(W3.T, dZ3), der_leakyrelu(Z2))\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), der_leakyrelu(Z1))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}    \n",
    "    return grads\n",
    "    #return {'dW1':dW1, 'db1':db1, 'dW2':dW2, 'db2':db2, 'dW3':dW3,'db3':db3}\n",
    "    #return dW1,db1,dW2,db2,dW3,db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(  dW1=0,db1=0,dW2=0,db2=0,dW3=0,db3=0) :\n",
    "    global W1,b1,W2,b2,W3,b3\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W3 = W3 - learning_rate * dW3\n",
    "    b3 = b3 - learning_rate * db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,A3) :\n",
    "    #A3,_,__ = forward_p(X)\n",
    "    predictions = A3 >0.5\n",
    "    predictions = np.where(A3 > 0.5, 1, 0)\n",
    "    return predictions\n",
    "\n",
    "def print_accuracy(X, Y, A3, train) :\n",
    "    predictions = predict(X, A3)\n",
    "    if train ==1 :\n",
    "        print(\"\\tTraining Accuracy: %.10f\" %(100 - np.mean(np.abs(predictions - Y)) * 100)+'%',end='')\n",
    "    else :\n",
    "        print(\"\\tValidation Accuracy: %.10f\" %(100 - np.mean(np.abs(predictions - Y)) * 100)+'%')\n",
    "    \n",
    "    return (100 - np.mean(np.abs(predictions - Y)) * 100)\n",
    "    #print('Accuracy: %f' %(float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T)))/float(Y.size)*100)+ '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_index = []\n",
    "l_train_cost_list = []\n",
    "l_val_cost_list = []\n",
    "l_ac_train=[]\n",
    "l_ac_val= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest=0\n",
    "highestindex=0\n",
    "highestloss=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_leakyrelu(X, Y, n_h, num_iterations , X_test, Y_test) :\n",
    "    global highest, highesindex, learning_rate, highestloss\n",
    "    init_var()\n",
    "    for i in range(0, num_iterations) :\n",
    "        A3 ,A2, A1, Z2, Z1= forward_p(X)\n",
    "        cost = compute_cost(A3, Y)\n",
    "        grads = back_p(X, Y, A3, A2, A1, Z2, Z1)\n",
    "        parameters = update_parameters( **grads)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if i % 1 == 0:\n",
    "            val_A3,_,__,_____,___=forward_p(X_test)\n",
    "            val_cost=compute_cost(val_A3,Y_test)\n",
    "            print(\"#%i Training loss : %.10f\\tValidation loss : %.10f\" %(i,cost,val_cost),end='')\n",
    "            l_train_ac = print_accuracy(X,Y,A3,1)\n",
    "            l_val_ac = print_accuracy(X_test,Y_test,val_A3,0)\n",
    "            \n",
    "            l_index.append(i)\n",
    "            l_train_cost_list.append(cost)\n",
    "            l_val_cost_list.append(val_cost)\n",
    "            l_ac_train.append(l_train_ac)\n",
    "            l_ac_val.append(l_val_ac)\n",
    "                \n",
    "        if i > 300 :\n",
    "            if l_train_cost_list[-1] == l_train_cost_list[-2] : \n",
    "                break\n",
    "                \n",
    "        if cost< 0.09:\n",
    "            learning_rate=0.1\n",
    "        if cost<0.05:\n",
    "            learning_rate=0.05\n",
    "        if cost<0.01:\n",
    "            learning_rate=0.01\n",
    "        if highest < l_val_ac :\n",
    "            highest = l_val_ac\n",
    "            highestindex=i\n",
    "            highestloss=val_cost\n",
    "            \n",
    "    return l_index,l_train_cost_list,l_val_cost_list,l_ac_train,l_ac_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Training loss : 0.6924548116\tValidation loss : 0.1724291311\tTraining Accuracy: 51.5092502434%\tValidation Accuracy: 50.0000000000%\n",
      "#1 Training loss : 0.6919583236\tValidation loss : 0.1723672558\tTraining Accuracy: 51.3145082765%\tValidation Accuracy: 50.0000000000%\n",
      "#2 Training loss : 0.6915217823\tValidation loss : 0.1723118400\tTraining Accuracy: 51.3145082765%\tValidation Accuracy: 50.0000000000%\n",
      "#3 Training loss : 0.6911244127\tValidation loss : 0.1722594692\tTraining Accuracy: 51.3145082765%\tValidation Accuracy: 50.0000000000%\n",
      "#4 Training loss : 0.6907506367\tValidation loss : 0.1722074368\tTraining Accuracy: 51.3145082765%\tValidation Accuracy: 50.0000000000%\n",
      "#5 Training loss : 0.6903892901\tValidation loss : 0.1721537286\tTraining Accuracy: 51.3145082765%\tValidation Accuracy: 50.0000000000%\n",
      "#6 Training loss : 0.6900320842\tValidation loss : 0.1720976871\tTraining Accuracy: 51.3145082765%\tValidation Accuracy: 50.0000000000%\n"
     ]
    }
   ],
   "source": [
    "#leakyrelu\n",
    "learning_rate=0.5\n",
    "l_index,l_train_cost_list,l_val_cost_list,l_ac_train,l_ac_val = nn_leakyrelu(x_train, y_train ,n_h, 2000 , x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"leakyrelu\")\n",
    "    plt.plot(l_index,l_train_cost_list)\n",
    "    #plt.xticks(l_index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Loss of Training Set\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(l_index,l_val_cost_list)\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Loss of Validation Set\")\n",
    "    plt.show()\n",
    "                          \n",
    "    plt.plot(l_index,l_ac_train)\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Accuracy of Training Set\")\n",
    "    plt.show()\n",
    "                            \n",
    "    plt.plot(l_index,l_ac_val)\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Accuracy of Validation Set\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headerColor = 'grey'\n",
    "rowEvenColor = 'lightgrey'\n",
    "rowOddColor = 'white'\n",
    "\n",
    "fig = go.Figure(data=[go.Table(\n",
    "header=dict(\n",
    "values=['<b></b>','<b>training</b>','<b>validation</b>','<b>best</b>'],\n",
    "line_color='darkslategray',\n",
    "fill_color=headerColor,\n",
    "align=['left','center'],\n",
    "font=dict(color='white', size=12)),\n",
    "cells=dict(\n",
    "values=[\n",
    "    ['loss', 'accuracy'],\n",
    "    [\"{0:.10f}\".format(l_train_cost_list[-1]), \"{0:.10f}\".format(l_ac_train[-1])],\n",
    "    [\"{0:.10f}\".format(l_val_cost_list[-1]), \"{0:.10f}\".format(l_ac_val[-1])],\n",
    "    [\"{0:.10f}\".format(highestloss),\"{0:.10f}\".format(highest)]],\n",
    "    line_color='darkslategray',\n",
    "    # 2-D list of colors for alternating rows\n",
    "#align = ['left', 'center'],\n",
    "font = dict(color = 'darkslategray', size = 11)))])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
