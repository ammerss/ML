{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "\n",
    "transform = transforms.Compose([#transforms.Resize((256,256)),  \n",
    "                                transforms.Grayscale(),\t\t# the code transforms.Graysclae() is for changing the size [3,100,100] to [1, 100, 100] (notice : [channel, height, width] )\n",
    "                                transforms.ToTensor(),])\n",
    "\n",
    "\n",
    "#train_data_path = 'relative path of training data set'\n",
    "train_data_path = 'horse-or-human/horse-or-human/train'\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "# if shuffle=True, the data reshuffled at every epoch #numworks가 뭐지?\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1027, shuffle=False, num_workers=0)  \n",
    "\n",
    "\n",
    "validation_data_path = 'horse-or-human/horse-or-human/validation'\n",
    "valset = torchvision.datasets.ImageFolder(root=validation_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=256, shuffle=False, num_workers=0)  \n",
    "\n",
    "NUM_EPOCH=1\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    # load training images of the batch size for every iteration\n",
    "    for i, data in enumerate(trainloader):\n",
    "\n",
    "        # inputs is the image\n",
    "        # labels is the class of the image\n",
    "        inputs, labels = data\n",
    "\n",
    "        # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "        #print(labels)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # load validation images of the batch size for every iteration\n",
    "    for i, data in enumerate(valloader):\n",
    "        \n",
    "        # inputs is the image\n",
    "        # labels is the class of the image\n",
    "        inputs, labels = data\n",
    "\n",
    "        # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "        #print(labels)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = next(iter(trainloader))[0].numpy()\n",
    "train_label =next(iter(trainloader))[1].numpy()\n",
    "\n",
    "val_dataset = next(iter(valloader))[0].numpy()\n",
    "val_label =next(iter(valloader))[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=train_label.reshape(1027,1)\n",
    "val_label=val_label.reshape(256,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flatten=train_dataset.reshape(1027,train_dataset.shape[2]*train_dataset.shape[3])\n",
    "val_flatten=val_dataset.reshape(256,val_dataset.shape[2]*val_dataset.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train_flatten.T\n",
    "y_train=train_label.T\n",
    "\n",
    "x_test=val_flatten.T\n",
    "y_test=val_label.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = x_train.shape[0] # size of input layer`\n",
    "#n_h=10 # size of hidden layer\n",
    "n_y = y_train.shape[0] # size of output layer\n",
    "m = x_train.shape[1]\n",
    "a =0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_h = 500 # hidden layer size\n",
    "#learning_rate=0.01\n",
    "\n",
    "W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "b1 = np.zeros(shape=(n_h, 1))\n",
    "\n",
    "W2 = np.random.randn(n_h,n_h) * 0.01\n",
    "b2 = np.zeros(shape=(n_h, 1))\n",
    "\n",
    "W3 = np.random.randn(n_y,n_h) * 0.01\n",
    "b3 = np.zeros(shape=(n_y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate variables\n",
    "def init_var() : \n",
    "    global W1,b1,W2,b2,W3,b3\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "    b1 = np.zeros(shape=(n_h, 1))\n",
    "\n",
    "    W2 = np.random.randn(n_h,n_h) * 0.01\n",
    "    b2 = np.zeros(shape=(n_h, 1))\n",
    "\n",
    "    W3 = np.random.randn(n_y,n_h) * 0.01\n",
    "    b3 = np.zeros(shape=(n_y, 1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    y = 1/(1+np.exp(-z))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_p(x_train):\n",
    "    #print(parameters)\n",
    "    Z1 = np.dot(W1,x_train) + b1\n",
    "    A1 = leakyrelu(Z1) # activation function\n",
    "    \n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = leakyrelu(Z2)  \n",
    "    \n",
    "    Z3 = np.dot(W3,A2) + b3\n",
    "    A3 = sigmoid(Z3) # Final output prediction\n",
    "    return A3, A2 , A1, Z2, Z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cross-entropy cost\n",
    "def compute_cost(A3, Y):\n",
    "    loss = np.multiply(np.log(A3), Y) + np.multiply((1 - Y), np.log(1 - A3))\n",
    "    cost = - np.sum(loss) / m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakyrelu(z):\n",
    "    return np.where(z > 0, z, z * a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def der_leakyrelu(z):\n",
    "    return np.where(z <= 0, 0, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_p( X, Y, A3, A2, A1, Z2 ,Z1) :\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = (1 / m) * np.dot(dZ3, A2.T)\n",
    "    db3 = (1 / m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    dZ2 = np.multiply(np.dot(W3.T, dZ3), der_leakyrelu(Z2))\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), der_leakyrelu(Z1))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}    \n",
    "    return grads\n",
    "    #return {'dW1':dW1, 'db1':db1, 'dW2':dW2, 'db2':db2, 'dW3':dW3,'db3':db3}\n",
    "    #return dW1,db1,dW2,db2,dW3,db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(  dW1=0,db1=0,dW2=0,db2=0,dW3=0,db3=0) :\n",
    "    global W1,b1,W2,b2,W3,b3\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W3 = W3 - learning_rate * dW3\n",
    "    b3 = b3 - learning_rate * db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,A3) :\n",
    "    #A3,_,__ = forward_p(X)\n",
    "    predictions = A3 >0.5\n",
    "    predictions = np.where(A3 > 0.5, 1, 0)\n",
    "    return predictions\n",
    "\n",
    "def print_accuracy(X, Y, A3, train) :\n",
    "    predictions = predict(X, A3)\n",
    "    if train ==1 :\n",
    "        print(\"\\tTraining Accuracy: %.10f\" %(100 - np.mean(np.abs(predictions - Y)) * 100)+'%',end='')\n",
    "    else :\n",
    "        print(\"\\tValidation Accuracy: %.10f\" %(100 - np.mean(np.abs(predictions - Y)) * 100)+'%')\n",
    "    \n",
    "    return (100 - np.mean(np.abs(predictions - Y)) * 100)\n",
    "    #print('Accuracy: %f' %(float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T)))/float(Y.size)*100)+ '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_index = []\n",
    "l_train_cost_list = []\n",
    "l_val_cost_list = []\n",
    "l_ac_train=[]\n",
    "l_ac_val= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest=0\n",
    "highestindex=0\n",
    "highestloss=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_leakyrelu(X, Y, n_h, num_iterations , X_test, Y_test) :\n",
    "    global highest, highesindex, learning_rate, highestloss\n",
    "    init_var()\n",
    "    for i in range(0, num_iterations) :\n",
    "        A3 ,A2, A1, Z2, Z1= forward_p(X)\n",
    "        cost = compute_cost(A3, Y)\n",
    "        grads = back_p(X, Y, A3, A2, A1, Z2, Z1)\n",
    "        parameters = update_parameters( **grads)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if i % 1 == 0:\n",
    "            val_A3,_,__,_____,___=forward_p(X_test)\n",
    "            val_cost=compute_cost(val_A3,Y_test)\n",
    "            print(\"#%i Training loss : %.10f\\tValidation loss : %.10f\" %(i,cost,val_cost),end='')\n",
    "            l_train_ac = print_accuracy(X,Y,A3,1)\n",
    "            l_val_ac = print_accuracy(X_test,Y_test,val_A3,0)\n",
    "            \n",
    "            l_index.append(i)\n",
    "            l_train_cost_list.append(cost)\n",
    "            l_val_cost_list.append(val_cost)\n",
    "            l_ac_train.append(l_train_ac)\n",
    "            l_ac_val.append(l_val_ac)\n",
    "                \n",
    "        if i > 300 :\n",
    "            if l_train_cost_list[-1] == l_train_cost_list[-2] : \n",
    "                break\n",
    "                \n",
    "        if cost< 0.04:\n",
    "            learning_rate=0.1\n",
    "        if cost<0.01:\n",
    "            learning_rate=0.01\n",
    "        if highest < l_val_ac :\n",
    "            highest = l_val_ac\n",
    "            highestindex=i\n",
    "            highestloss=val_cost\n",
    "            \n",
    "    return l_index,l_train_cost_list,l_val_cost_list,l_ac_train,l_ac_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Training loss : 0.6932221451\tValidation loss : 0.1725508910\tTraining Accuracy: 54.1382667965%\tValidation Accuracy: 50.0000000000%\n",
      "#1 Training loss : 0.6912234012\tValidation loss : 0.1724912936\tTraining Accuracy: 51.3145082765%\tValidation Accuracy: 50.0000000000%\n",
      "#2 Training loss : 0.6898839101\tValidation loss : 0.1724277785\tTraining Accuracy: 51.3145082765%\tValidation Accuracy: 50.0000000000%\n",
      "#3 Training loss : 0.6888021027\tValidation loss : 0.1723284993\tTraining Accuracy: 51.3145082765%\tValidation Accuracy: 50.0000000000%\n",
      "#4 Training loss : 0.6878180505\tValidation loss : 0.1721899117\tTraining Accuracy: 51.3145082765%\tValidation Accuracy: 50.0000000000%\n",
      "#5 Training loss : 0.6868622035\tValidation loss : 0.1720168788\tTraining Accuracy: 51.3145082765%\tValidation Accuracy: 50.0000000000%\n",
      "#6 Training loss : 0.6859028619\tValidation loss : 0.1718148028\tTraining Accuracy: 51.3145082765%\tValidation Accuracy: 50.0000000000%\n",
      "#7 Training loss : 0.6849257907\tValidation loss : 0.1715880622\tTraining Accuracy: 51.3145082765%\tValidation Accuracy: 50.0000000000%\n",
      "#8 Training loss : 0.6839214948\tValidation loss : 0.1713392586\tTraining Accuracy: 51.3145082765%\tValidation Accuracy: 50.0000000000%\n",
      "#9 Training loss : 0.6828841202\tValidation loss : 0.1710698373\tTraining Accuracy: 51.4118792600%\tValidation Accuracy: 50.0000000000%\n",
      "#10 Training loss : 0.6818086998\tValidation loss : 0.1707805813\tTraining Accuracy: 51.4118792600%\tValidation Accuracy: 50.0000000000%\n",
      "#11 Training loss : 0.6806904535\tValidation loss : 0.1704705860\tTraining Accuracy: 51.8987341772%\tValidation Accuracy: 50.0000000000%\n",
      "#12 Training loss : 0.6795256243\tValidation loss : 0.1701378900\tTraining Accuracy: 53.2619279455%\tValidation Accuracy: 50.0000000000%\n",
      "#13 Training loss : 0.6783099154\tValidation loss : 0.1697816397\tTraining Accuracy: 54.6251217137%\tValidation Accuracy: 50.0000000000%\n",
      "#14 Training loss : 0.6770392677\tValidation loss : 0.1693970961\tTraining Accuracy: 55.3067185979%\tValidation Accuracy: 50.0000000000%\n",
      "#15 Training loss : 0.6757085475\tValidation loss : 0.1689830642\tTraining Accuracy: 55.1119766310%\tValidation Accuracy: 50.0000000000%\n",
      "#16 Training loss : 0.6743126827\tValidation loss : 0.1685352446\tTraining Accuracy: 55.9883154820%\tValidation Accuracy: 50.0000000000%\n",
      "#17 Training loss : 0.6728457977\tValidation loss : 0.1680551370\tTraining Accuracy: 56.8646543330%\tValidation Accuracy: 50.0000000000%\n",
      "#18 Training loss : 0.6713037254\tValidation loss : 0.1675366663\tTraining Accuracy: 57.1567672833%\tValidation Accuracy: 50.3906250000%\n",
      "#19 Training loss : 0.6696836128\tValidation loss : 0.1669794962\tTraining Accuracy: 57.6436222006%\tValidation Accuracy: 50.3906250000%\n",
      "#20 Training loss : 0.6679807109\tValidation loss : 0.1663808287\tTraining Accuracy: 57.7409931840%\tValidation Accuracy: 50.3906250000%\n",
      "#21 Training loss : 0.6661879210\tValidation loss : 0.1657373132\tTraining Accuracy: 57.7409931840%\tValidation Accuracy: 50.3906250000%\n",
      "#22 Training loss : 0.6643003043\tValidation loss : 0.1650447383\tTraining Accuracy: 57.6436222006%\tValidation Accuracy: 50.3906250000%\n",
      "#23 Training loss : 0.6623153541\tValidation loss : 0.1642984997\tTraining Accuracy: 58.3252190847%\tValidation Accuracy: 51.1718750000%\n",
      "#24 Training loss : 0.6602260570\tValidation loss : 0.1634918805\tTraining Accuracy: 58.6173320351%\tValidation Accuracy: 51.1718750000%\n",
      "#25 Training loss : 0.6580245907\tValidation loss : 0.1626255577\tTraining Accuracy: 59.0068159688%\tValidation Accuracy: 52.7343750000%\n",
      "#26 Training loss : 0.6557072728\tValidation loss : 0.1616944296\tTraining Accuracy: 59.2015579357%\tValidation Accuracy: 53.1250000000%\n",
      "#27 Training loss : 0.6532672395\tValidation loss : 0.1606958468\tTraining Accuracy: 59.7857838364%\tValidation Accuracy: 54.2968750000%\n",
      "#28 Training loss : 0.6506985659\tValidation loss : 0.1596243912\tTraining Accuracy: 60.8568646543%\tValidation Accuracy: 56.6406250000%\n",
      "#29 Training loss : 0.6479957102\tValidation loss : 0.1584855603\tTraining Accuracy: 61.6358325219%\tValidation Accuracy: 57.4218750000%\n",
      "#30 Training loss : 0.6451544979\tValidation loss : 0.1572661536\tTraining Accuracy: 62.2200584226%\tValidation Accuracy: 58.5937500000%\n",
      "#31 Training loss : 0.6421659908\tValidation loss : 0.1559667216\tTraining Accuracy: 62.8042843233%\tValidation Accuracy: 60.9375000000%\n",
      "#32 Training loss : 0.6390208452\tValidation loss : 0.1545750282\tTraining Accuracy: 63.7779941577%\tValidation Accuracy: 62.5000000000%\n",
      "#33 Training loss : 0.6357130125\tValidation loss : 0.1531053066\tTraining Accuracy: 65.1411879260%\tValidation Accuracy: 66.0156250000%\n",
      "#34 Training loss : 0.6322410040\tValidation loss : 0.1515410043\tTraining Accuracy: 65.6280428432%\tValidation Accuracy: 67.5781250000%\n",
      "#35 Training loss : 0.6285966136\tValidation loss : 0.1498956044\tTraining Accuracy: 66.3096397274%\tValidation Accuracy: 69.9218750000%\n",
      "#36 Training loss : 0.6247872483\tValidation loss : 0.1481654393\tTraining Accuracy: 66.6991236611%\tValidation Accuracy: 73.8281250000%\n",
      "#37 Training loss : 0.6208151238\tValidation loss : 0.1463484629\tTraining Accuracy: 67.8675754625%\tValidation Accuracy: 75.0000000000%\n",
      "#38 Training loss : 0.6166784054\tValidation loss : 0.1444573627\tTraining Accuracy: 68.1596884129%\tValidation Accuracy: 76.1718750000%\n",
      "#39 Training loss : 0.6123807580\tValidation loss : 0.1424747626\tTraining Accuracy: 68.8412852970%\tValidation Accuracy: 77.7343750000%\n",
      "#40 Training loss : 0.6079365639\tValidation loss : 0.1404288437\tTraining Accuracy: 69.6202531646%\tValidation Accuracy: 80.0781250000%\n",
      "#41 Training loss : 0.6033456366\tValidation loss : 0.1383074821\tTraining Accuracy: 70.4965920156%\tValidation Accuracy: 82.0312500000%\n",
      "#42 Training loss : 0.5986202179\tValidation loss : 0.1361227155\tTraining Accuracy: 71.1781888997%\tValidation Accuracy: 83.2031250000%\n",
      "#43 Training loss : 0.5937586709\tValidation loss : 0.1338910902\tTraining Accuracy: 71.6650438169%\tValidation Accuracy: 83.2031250000%\n",
      "#44 Training loss : 0.5887768612\tValidation loss : 0.1316029316\tTraining Accuracy: 71.9571567673%\tValidation Accuracy: 83.5937500000%\n",
      "#45 Training loss : 0.5836916256\tValidation loss : 0.1293017733\tTraining Accuracy: 72.1518987342%\tValidation Accuracy: 85.1562500000%\n",
      "#46 Training loss : 0.5785221276\tValidation loss : 0.1269607794\tTraining Accuracy: 72.6387536514%\tValidation Accuracy: 85.1562500000%\n",
      "#47 Training loss : 0.5732904862\tValidation loss : 0.1246386151\tTraining Accuracy: 72.6387536514%\tValidation Accuracy: 84.7656250000%\n",
      "#48 Training loss : 0.5680206120\tValidation loss : 0.1222759176\tTraining Accuracy: 72.8334956183%\tValidation Accuracy: 85.9375000000%\n",
      "#49 Training loss : 0.5627293670\tValidation loss : 0.1200063122\tTraining Accuracy: 73.4177215190%\tValidation Accuracy: 86.3281250000%\n",
      "#50 Training loss : 0.5574289667\tValidation loss : 0.1176262554\tTraining Accuracy: 73.7098344693%\tValidation Accuracy: 87.1093750000%\n",
      "#51 Training loss : 0.5521432525\tValidation loss : 0.1155584290\tTraining Accuracy: 73.8072054528%\tValidation Accuracy: 87.1093750000%\n",
      "#52 Training loss : 0.5468959626\tValidation loss : 0.1130243653\tTraining Accuracy: 73.9045764362%\tValidation Accuracy: 86.7187500000%\n",
      "#53 Training loss : 0.5417326506\tValidation loss : 0.1118384069\tTraining Accuracy: 74.4888023369%\tValidation Accuracy: 86.7187500000%\n",
      "#54 Training loss : 0.5368862377\tValidation loss : 0.1090924090\tTraining Accuracy: 74.0019474197%\tValidation Accuracy: 85.1562500000%\n",
      "#55 Training loss : 0.5337469816\tValidation loss : 0.1206189215\tTraining Accuracy: 75.4625121714%\tValidation Accuracy: 79.6875000000%\n",
      "#56 Training loss : 0.5441316192\tValidation loss : 0.1747416304\tTraining Accuracy: 69.2307692308%\tValidation Accuracy: 52.7343750000%\n",
      "#57 Training loss : 0.6570801433\tValidation loss : 0.4903367193\tTraining Accuracy: 54.6251217137%\tValidation Accuracy: 50.0000000000%\n",
      "#58 Training loss : 1.1722738928\tValidation loss : 0.2138956264\tTraining Accuracy: 51.3145082765%\tValidation Accuracy: 50.0000000000%\n",
      "#59 Training loss : 0.7716444787\tValidation loss : 0.1349621595\tTraining Accuracy: 48.6854917235%\tValidation Accuracy: 81.2500000000%\n",
      "#60 Training loss : 0.5883264543\tValidation loss : 0.1310324736\tTraining Accuracy: 72.0545277507%\tValidation Accuracy: 81.2500000000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#61 Training loss : 0.5780724214\tValidation loss : 0.1246994580\tTraining Accuracy: 70.8860759494%\tValidation Accuracy: 84.3750000000%\n",
      "#62 Training loss : 0.5672720382\tValidation loss : 0.1187291707\tTraining Accuracy: 72.6387536514%\tValidation Accuracy: 85.5468750000%\n",
      "#63 Training loss : 0.5557363371\tValidation loss : 0.1130304922\tTraining Accuracy: 73.3203505355%\tValidation Accuracy: 87.1093750000%\n",
      "#64 Training loss : 0.5439571701\tValidation loss : 0.1083832498\tTraining Accuracy: 74.0993184031%\tValidation Accuracy: 86.7187500000%\n",
      "#65 Training loss : 0.5329094407\tValidation loss : 0.1049278704\tTraining Accuracy: 74.4888023369%\tValidation Accuracy: 86.3281250000%\n",
      "#66 Training loss : 0.5233285941\tValidation loss : 0.1024372990\tTraining Accuracy: 74.8782862707%\tValidation Accuracy: 87.1093750000%\n",
      "#67 Training loss : 0.5157339170\tValidation loss : 0.1004133789\tTraining Accuracy: 75.3651411879%\tValidation Accuracy: 87.1093750000%\n",
      "#68 Training loss : 0.5098305471\tValidation loss : 0.0985389823\tTraining Accuracy: 75.5598831548%\tValidation Accuracy: 87.1093750000%\n",
      "#69 Training loss : 0.5046200354\tValidation loss : 0.0969075291\tTraining Accuracy: 75.6572541383%\tValidation Accuracy: 87.1093750000%\n",
      "#70 Training loss : 0.4997650987\tValidation loss : 0.0953275514\tTraining Accuracy: 75.5598831548%\tValidation Accuracy: 85.9375000000%\n",
      "#71 Training loss : 0.4952866248\tValidation loss : 0.0946296451\tTraining Accuracy: 75.5598831548%\tValidation Accuracy: 86.7187500000%\n",
      "#72 Training loss : 0.4920026201\tValidation loss : 0.0986410650\tTraining Accuracy: 75.0730282376%\tValidation Accuracy: 82.8125000000%\n",
      "#73 Training loss : 0.4970796885\tValidation loss : 0.1432817947\tTraining Accuracy: 77.3125608569%\tValidation Accuracy: 71.4843750000%\n",
      "#74 Training loss : 0.5738213383\tValidation loss : 0.3634238673\tTraining Accuracy: 67.5754625122%\tValidation Accuracy: 50.0000000000%\n",
      "#75 Training loss : 1.0179862126\tValidation loss : 0.4888316743\tTraining Accuracy: 48.7828627069%\tValidation Accuracy: 50.0000000000%\n",
      "#76 Training loss : 1.1732520504\tValidation loss : 0.1330980140\tTraining Accuracy: 51.3145082765%\tValidation Accuracy: 85.1562500000%\n",
      "#77 Training loss : 0.5972269424\tValidation loss : 0.1271975830\tTraining Accuracy: 77.9941577410%\tValidation Accuracy: 83.9843750000%\n",
      "#78 Training loss : 0.5717499498\tValidation loss : 0.1233055081\tTraining Accuracy: 73.6124634859%\tValidation Accuracy: 84.3750000000%\n",
      "#79 Training loss : 0.5605505363\tValidation loss : 0.1171511004\tTraining Accuracy: 72.7361246349%\tValidation Accuracy: 85.9375000000%\n",
      "#80 Training loss : 0.5487999469\tValidation loss : 0.1106344972\tTraining Accuracy: 73.8072054528%\tValidation Accuracy: 85.5468750000%\n",
      "#81 Training loss : 0.5360562798\tValidation loss : 0.1044802648\tTraining Accuracy: 74.7809152872%\tValidation Accuracy: 86.7187500000%\n",
      "#82 Training loss : 0.5228428875\tValidation loss : 0.0995777854\tTraining Accuracy: 74.9756572541%\tValidation Accuracy: 87.1093750000%\n",
      "#83 Training loss : 0.5103993196\tValidation loss : 0.0960367743\tTraining Accuracy: 75.4625121714%\tValidation Accuracy: 87.1093750000%\n",
      "#84 Training loss : 0.4996031382\tValidation loss : 0.0934709178\tTraining Accuracy: 75.7546251217%\tValidation Accuracy: 86.7187500000%\n",
      "#85 Training loss : 0.4908677383\tValidation loss : 0.0914415229\tTraining Accuracy: 75.7546251217%\tValidation Accuracy: 86.7187500000%\n",
      "#86 Training loss : 0.4840464828\tValidation loss : 0.0898205064\tTraining Accuracy: 75.9493670886%\tValidation Accuracy: 85.9375000000%\n",
      "#87 Training loss : 0.4784089082\tValidation loss : 0.0884124541\tTraining Accuracy: 76.2414800389%\tValidation Accuracy: 87.1093750000%\n",
      "#88 Training loss : 0.4736368383\tValidation loss : 0.0880882103\tTraining Accuracy: 76.1441090555%\tValidation Accuracy: 85.1562500000%\n",
      "#89 Training loss : 0.4702597825\tValidation loss : 0.0896410617\tTraining Accuracy: 77.0204479065%\tValidation Accuracy: 87.5000000000%\n",
      "#90 Training loss : 0.4726706490\tValidation loss : 0.1143464307\tTraining Accuracy: 75.8519961052%\tValidation Accuracy: 75.7812500000%\n",
      "#91 Training loss : 0.5097501188\tValidation loss : 0.2433041370\tTraining Accuracy: 74.1966893866%\tValidation Accuracy: 57.4218750000%\n",
      "#92 Training loss : 0.7436552990\tValidation loss : 0.3483681704\tTraining Accuracy: 59.6884128530%\tValidation Accuracy: 50.0000000000%\n",
      "#93 Training loss : 0.9929054539\tValidation loss : 0.2149124699\tTraining Accuracy: 48.9776046738%\tValidation Accuracy: 53.9062500000%\n",
      "#94 Training loss : 0.6797456908\tValidation loss : 0.1116425240\tTraining Accuracy: 58.7147030185%\tValidation Accuracy: 84.7656250000%\n",
      "#95 Training loss : 0.5407628355\tValidation loss : 0.1064132087\tTraining Accuracy: 77.8967867575%\tValidation Accuracy: 85.9375000000%\n",
      "#96 Training loss : 0.5201012789\tValidation loss : 0.0994143600\tTraining Accuracy: 75.6572541383%\tValidation Accuracy: 87.5000000000%\n",
      "#97 Training loss : 0.5061707873\tValidation loss : 0.0946080031\tTraining Accuracy: 75.7546251217%\tValidation Accuracy: 87.1093750000%\n",
      "#98 Training loss : 0.4931706837\tValidation loss : 0.0908323624\tTraining Accuracy: 76.3388510224%\tValidation Accuracy: 86.3281250000%\n",
      "#99 Training loss : 0.4819028788\tValidation loss : 0.0881727893\tTraining Accuracy: 76.5335929893%\tValidation Accuracy: 86.3281250000%\n",
      "#100 Training loss : 0.4724841893\tValidation loss : 0.0863327124\tTraining Accuracy: 77.2151898734%\tValidation Accuracy: 86.7187500000%\n",
      "#101 Training loss : 0.4649837233\tValidation loss : 0.0846089752\tTraining Accuracy: 77.1178188900%\tValidation Accuracy: 86.3281250000%\n",
      "#102 Training loss : 0.4589479958\tValidation loss : 0.0839564248\tTraining Accuracy: 77.2151898734%\tValidation Accuracy: 85.1562500000%\n",
      "#103 Training loss : 0.4542192322\tValidation loss : 0.0829087016\tTraining Accuracy: 77.7020447907%\tValidation Accuracy: 87.1093750000%\n",
      "#104 Training loss : 0.4515979995\tValidation loss : 0.0898891385\tTraining Accuracy: 76.6309639727%\tValidation Accuracy: 83.5937500000%\n",
      "#105 Training loss : 0.4571839303\tValidation loss : 0.1139851224\tTraining Accuracy: 78.3836416748%\tValidation Accuracy: 82.4218750000%\n",
      "#106 Training loss : 0.5108913654\tValidation loss : 0.2501560192\tTraining Accuracy: 72.9308666018%\tValidation Accuracy: 52.7343750000%\n",
      "#107 Training loss : 0.7487361299\tValidation loss : 0.4774019959\tTraining Accuracy: 54.6251217137%\tValidation Accuracy: 50.0000000000%\n",
      "#108 Training loss : 1.1569629965\tValidation loss : 0.1180059089\tTraining Accuracy: 52.3855890944%\tValidation Accuracy: 78.9062500000%\n",
      "#109 Training loss : 0.5595264582\tValidation loss : 0.0998152518\tTraining Accuracy: 73.4177215190%\tValidation Accuracy: 86.7187500000%\n",
      "#110 Training loss : 0.5069416373\tValidation loss : 0.0935349201\tTraining Accuracy: 76.4362220058%\tValidation Accuracy: 87.5000000000%\n",
      "#111 Training loss : 0.4907382025\tValidation loss : 0.0890288695\tTraining Accuracy: 77.1178188900%\tValidation Accuracy: 86.7187500000%\n",
      "#112 Training loss : 0.4762785401\tValidation loss : 0.0859560013\tTraining Accuracy: 77.7994157741%\tValidation Accuracy: 85.9375000000%\n",
      "#113 Training loss : 0.4643296938\tValidation loss : 0.0837446131\tTraining Accuracy: 78.1888997079%\tValidation Accuracy: 86.3281250000%\n",
      "#114 Training loss : 0.4548952952\tValidation loss : 0.0821453118\tTraining Accuracy: 78.3836416748%\tValidation Accuracy: 86.3281250000%\n",
      "#115 Training loss : 0.4475939571\tValidation loss : 0.0808636246\tTraining Accuracy: 78.3836416748%\tValidation Accuracy: 86.3281250000%\n",
      "#116 Training loss : 0.4417165108\tValidation loss : 0.0799802508\tTraining Accuracy: 78.6757546251%\tValidation Accuracy: 85.5468750000%\n",
      "#117 Training loss : 0.4368160622\tValidation loss : 0.0790138323\tTraining Accuracy: 78.5783836417%\tValidation Accuracy: 86.3281250000%\n",
      "#118 Training loss : 0.4325563401\tValidation loss : 0.0789292792\tTraining Accuracy: 78.4810126582%\tValidation Accuracy: 85.1562500000%\n",
      "#119 Training loss : 0.4288848383\tValidation loss : 0.0776771021\tTraining Accuracy: 78.5783836417%\tValidation Accuracy: 85.9375000000%\n",
      "#120 Training loss : 0.4263224364\tValidation loss : 0.0837445922\tTraining Accuracy: 78.5783836417%\tValidation Accuracy: 85.5468750000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#121 Training loss : 0.4291973946\tValidation loss : 0.0976801649\tTraining Accuracy: 78.7731256086%\tValidation Accuracy: 85.5468750000%\n",
      "#122 Training loss : 0.4689837711\tValidation loss : 0.2430502021\tTraining Accuracy: 76.3388510224%\tValidation Accuracy: 55.8593750000%\n",
      "#123 Training loss : 0.7081078886\tValidation loss : 0.5713638955\tTraining Accuracy: 58.0331061344%\tValidation Accuracy: 50.0000000000%\n",
      "#124 Training loss : 1.3469571414\tValidation loss : 0.1116376285\tTraining Accuracy: 51.9961051607%\tValidation Accuracy: 82.8125000000%\n",
      "#125 Training loss : 0.5439402559\tValidation loss : 0.0967822639\tTraining Accuracy: 77.0204479065%\tValidation Accuracy: 87.8906250000%\n",
      "#126 Training loss : 0.4984035396\tValidation loss : 0.0908203403\tTraining Accuracy: 77.7020447907%\tValidation Accuracy: 87.1093750000%\n",
      "#127 Training loss : 0.4795120238\tValidation loss : 0.0861683810\tTraining Accuracy: 77.7994157741%\tValidation Accuracy: 86.3281250000%\n",
      "#128 Training loss : 0.4629760492\tValidation loss : 0.0828725972\tTraining Accuracy: 78.7731256086%\tValidation Accuracy: 86.3281250000%\n",
      "#129 Training loss : 0.4491724633\tValidation loss : 0.0805923386\tTraining Accuracy: 78.9678675755%\tValidation Accuracy: 85.9375000000%\n",
      "#130 Training loss : 0.4382599547\tValidation loss : 0.0789404343\tTraining Accuracy: 79.1626095424%\tValidation Accuracy: 85.9375000000%\n",
      "#131 Training loss : 0.4297489188\tValidation loss : 0.0778530446\tTraining Accuracy: 79.2599805258%\tValidation Accuracy: 85.9375000000%\n",
      "#132 Training loss : 0.4230881707\tValidation loss : 0.0769643050\tTraining Accuracy: 79.3573515093%\tValidation Accuracy: 85.5468750000%\n",
      "#133 Training loss : 0.4176765615\tValidation loss : 0.0765414244\tTraining Accuracy: 79.3573515093%\tValidation Accuracy: 85.9375000000%\n",
      "#134 Training loss : 0.4132304297\tValidation loss : 0.0756841390\tTraining Accuracy: 79.6494644596%\tValidation Accuracy: 85.5468750000%\n",
      "#135 Training loss : 0.4093947080\tValidation loss : 0.0766142470\tTraining Accuracy: 80.1363193768%\tValidation Accuracy: 85.9375000000%\n",
      "#136 Training loss : 0.4063073255\tValidation loss : 0.0749075583\tTraining Accuracy: 79.8442064265%\tValidation Accuracy: 86.7187500000%\n",
      "#137 Training loss : 0.4056399936\tValidation loss : 0.0924045640\tTraining Accuracy: 80.8179162610%\tValidation Accuracy: 82.4218750000%\n",
      "#138 Training loss : 0.4215147198\tValidation loss : 0.1446416898\tTraining Accuracy: 79.2599805258%\tValidation Accuracy: 77.3437500000%\n",
      "#139 Training loss : 0.5562393578\tValidation loss : 0.4130108780\tTraining Accuracy: 71.5676728335%\tValidation Accuracy: 50.0000000000%\n",
      "#140 Training loss : 1.0419954057\tValidation loss : 0.4893426885\tTraining Accuracy: 50.7302823759%\tValidation Accuracy: 50.0000000000%\n",
      "#141 Training loss : 1.1904178153\tValidation loss : 0.1160628461\tTraining Accuracy: 51.8987341772%\tValidation Accuracy: 87.5000000000%\n",
      "#142 Training loss : 0.5421376904\tValidation loss : 0.1110700398\tTraining Accuracy: 77.5073028238%\tValidation Accuracy: 86.7187500000%\n",
      "#143 Training loss : 0.5259650764\tValidation loss : 0.1047440992\tTraining Accuracy: 76.6309639727%\tValidation Accuracy: 87.5000000000%\n",
      "#144 Training loss : 0.5104608171\tValidation loss : 0.0978420187\tTraining Accuracy: 77.2151898734%\tValidation Accuracy: 87.8906250000%\n",
      "#145 Training loss : 0.4939736103\tValidation loss : 0.0916215613\tTraining Accuracy: 77.4099318403%\tValidation Accuracy: 88.6718750000%\n",
      "#146 Training loss : 0.4772075215\tValidation loss : 0.0867947448\tTraining Accuracy: 78.4810126582%\tValidation Accuracy: 86.3281250000%\n",
      "#147 Training loss : 0.4611420080\tValidation loss : 0.0833214028\tTraining Accuracy: 78.8704965920%\tValidation Accuracy: 86.3281250000%\n",
      "#148 Training loss : 0.4466902909\tValidation loss : 0.0811190058\tTraining Accuracy: 79.2599805258%\tValidation Accuracy: 85.9375000000%\n",
      "#149 Training loss : 0.4344737092\tValidation loss : 0.0796185286\tTraining Accuracy: 79.3573515093%\tValidation Accuracy: 85.9375000000%\n",
      "#150 Training loss : 0.4246111472\tValidation loss : 0.0784577210\tTraining Accuracy: 79.6494644596%\tValidation Accuracy: 85.9375000000%\n",
      "#151 Training loss : 0.4166004005\tValidation loss : 0.0776468671\tTraining Accuracy: 79.6494644596%\tValidation Accuracy: 85.9375000000%\n",
      "#152 Training loss : 0.4099789954\tValidation loss : 0.0768082835\tTraining Accuracy: 80.1363193768%\tValidation Accuracy: 85.9375000000%\n",
      "#153 Training loss : 0.4042841531\tValidation loss : 0.0764493791\tTraining Accuracy: 80.5258033106%\tValidation Accuracy: 85.5468750000%\n",
      "#154 Training loss : 0.3993768304\tValidation loss : 0.0754180059\tTraining Accuracy: 80.5258033106%\tValidation Accuracy: 85.9375000000%\n",
      "#155 Training loss : 0.3950533568\tValidation loss : 0.0765993344\tTraining Accuracy: 80.8179162610%\tValidation Accuracy: 85.5468750000%\n"
     ]
    }
   ],
   "source": [
    "#leakyrelu\n",
    "learning_rate=0.5\n",
    "l_index,l_train_cost_list,l_val_cost_list,l_ac_train,l_ac_val = nn_leakyrelu(x_train, y_train ,n_h, 9999 , x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"leakyrelu\")\n",
    "    plt.plot(l_index,l_train_cost_list)\n",
    "    #plt.xticks(l_index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Loss of Training Set\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(l_index,l_val_cost_list)\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Loss of Validation Set\")\n",
    "    plt.show()\n",
    "                          \n",
    "    plt.plot(l_index,l_ac_train)\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Accuracy of Training Set\")\n",
    "    plt.show()\n",
    "                            \n",
    "    plt.plot(l_index,l_ac_val)\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Accuracy of Validation Set\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headerColor = 'grey'\n",
    "rowEvenColor = 'lightgrey'\n",
    "rowOddColor = 'white'\n",
    "\n",
    "fig = go.Figure(data=[go.Table(\n",
    "header=dict(\n",
    "values=['<b></b>','<b>training</b>','<b>validation</b>','<b>best</b>'],\n",
    "line_color='darkslategray',\n",
    "fill_color=headerColor,\n",
    "align=['left','center'],\n",
    "font=dict(color='white', size=12)),\n",
    "cells=dict(\n",
    "values=[\n",
    "    ['loss', 'accuracy'],\n",
    "    [\"{0:.10f}\".format(l_train_cost_list[-1]), \"{0:.10f}\".format(l_ac_train[-1])],\n",
    "    [\"{0:.10f}\".format(l_val_cost_list[-1]), \"{0:.10f}\".format(l_ac_val[-1])],\n",
    "    [\"{0:.10f}\".format(highestloss),\"{0:.10f}\".format(highest)]],\n",
    "    line_color='darkslategray',\n",
    "    # 2-D list of colors for alternating rows\n",
    "#align = ['left', 'center'],\n",
    "font = dict(color = 'darkslategray', size = 11)))])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
