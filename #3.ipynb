{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "\n",
    "transform = transforms.Compose([#transforms.Resize((256,256)),  \n",
    "                                transforms.Grayscale(),\t\t# the code transforms.Graysclae() is for changing the size [3,100,100] to [1, 100, 100] (notice : [channel, height, width] )\n",
    "                                transforms.ToTensor(),])\n",
    "\n",
    "\n",
    "#train_data_path = 'relative path of training data set'\n",
    "train_data_path = 'horse-or-human/horse-or-human/train'\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "# if shuffle=True, the data reshuffled at every epoch #numworks가 뭐지?\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1027, shuffle=False, num_workers=0)  \n",
    "\n",
    "\n",
    "validation_data_path = 'horse-or-human/horse-or-human/validation'\n",
    "valset = torchvision.datasets.ImageFolder(root=validation_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=256, shuffle=False, num_workers=0)  \n",
    "\n",
    "NUM_EPOCH=1\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    # load training images of the batch size for every iteration\n",
    "    for i, data in enumerate(trainloader):\n",
    "\n",
    "        # inputs is the image\n",
    "        # labels is the class of the image\n",
    "        inputs, labels = data\n",
    "\n",
    "        # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "        #print(labels)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # load validation images of the batch size for every iteration\n",
    "    for i, data in enumerate(valloader):\n",
    "        \n",
    "        # inputs is the image\n",
    "        # labels is the class of the image\n",
    "        inputs, labels = data\n",
    "\n",
    "        # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "        #print(labels)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1027, 1, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = next(iter(trainloader))[0].numpy()\n",
    "train_label =next(iter(trainloader))[1].numpy()\n",
    "\n",
    "val_dataset = next(iter(valloader))[0].numpy()\n",
    "val_label =next(iter(valloader))[1].numpy()\n",
    "\n",
    "print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=train_label.reshape(1027,1)\n",
    "val_label=val_label.reshape(256,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flatten=train_dataset.reshape(1027,train_dataset.shape[2]*train_dataset.shape[3])\n",
    "val_flatten=val_dataset.reshape(256,val_dataset.shape[2]*val_dataset.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1027)\n",
      "(1, 1027)\n",
      "(10000, 256)\n",
      "(1, 256)\n"
     ]
    }
   ],
   "source": [
    "x_train=train_flatten.T\n",
    "y_train=train_label.T\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "x_test=val_flatten.T\n",
    "y_test=val_label.T\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 1\n"
     ]
    }
   ],
   "source": [
    "n_x = x_train.shape[0] # size of input layer`\n",
    " # size of hidden layer\n",
    "n_y = y_train.shape[0] # size of output layer\n",
    "print(n_x,n_y)\n",
    "m = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    y = 1/(1+np.exp(-z))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_p(x_train):\n",
    "    #print(parameters)\n",
    "    Z1 = np.dot(W1,x_train) + b1\n",
    "    A1 = np.tanh(Z1) # activation function\n",
    "    \n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = np.tanh(Z2)  #np.tanh 바꾼거\n",
    "    \n",
    "    Z3 = np.dot(W3,A2) + b3\n",
    "    A3 = sigmoid(Z3) # Final output prediction\n",
    "    return A3, A2 , A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cross-entropy cost\n",
    "def compute_cost(A3, Y):\n",
    "    loss = np.multiply(np.log(A3), Y) + np.multiply((1 - Y), np.log(1 - A3))\n",
    "    cost = - np.sum(loss) / m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_p( X, Y, A3, A2, A1) :\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = (1 / m) * np.dot(dZ3, A2.T)\n",
    "    db3 = (1 / m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    dZ2 = np.multiply(np.dot(W3.T, dZ3), 1 - np.power(A2, 2))\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}    \n",
    "    return grads\n",
    "    #return {'dW1':dW1, 'db1':db1, 'dW2':dW2, 'db2':db2, 'dW3':dW3,'db3':db3}\n",
    "    #return dW1,db1,dW2,db2,dW3,db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(  dW1=0,db1=0,dW2=0,db2=0,dW3=0,db3=0) :\n",
    "    global W1,b1,W2,b2,W3,b3,learning_rate\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W3 = W3 - learning_rate * dW3\n",
    "    b3 = b3 - learning_rate * db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,A3) :\n",
    "    #A3,_,__ = forward_p(X)\n",
    "    predictions = A3 >0.5\n",
    "    predictions = np.where(A3 > 0.5, 1, 0)\n",
    "    return predictions\n",
    "\n",
    "def print_accuracy(X, Y, A3, train) :\n",
    "    predictions = predict(X, A3)\n",
    "    if train ==1 :\n",
    "        print(\"\\tTraining Accuracy: %f\" %(round(100 - np.mean(np.abs(predictions - Y)) * 100,6))+'%',end='')\n",
    "    else :\n",
    "        print(\"\\tValidation Accuracy: %f\" %(round(100 - np.mean(np.abs(predictions - Y)) * 100,6))+'%')\n",
    "    \n",
    "    return (round(100 - np.mean(np.abs(predictions - Y)) * 100,6))\n",
    "    #print('Accuracy: %f' %(float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T)))/float(Y.size)*100)+ '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations , X_test, Y_test):\n",
    "\n",
    "    for i in range(0, num_iterations) :\n",
    "        A3 ,A2, A1= forward_p(X)\n",
    "        cost = compute_cost(A3, Y)\n",
    "        grads = back_p(X, Y, A3, A2, A1)\n",
    "        parameters = update_parameters( **grads)\n",
    "        \n",
    "        if i % 300 == 0:\n",
    "            val_A3,_,__=forward_p(X_test)\n",
    "            val_cost=compute_cost(val_A3,Y_test)\n",
    "            print(\"#%i Training loss : %f\\tValidation loss : %f\" %(i,cost,val_cost),end='')\n",
    "            train_ac = print_accuracy(X,Y,A3,1)\n",
    "            val_ac = print_accuracy(X_test,Y_test,val_A3,0)\n",
    "            \n",
    "            index.append(i)\n",
    "            train_cost_list.append(cost)\n",
    "            val_cost_list.append(val_cost)\n",
    "            #print(\"!!!!!\",train_ac)\n",
    "            ac_train.append(train_ac)\n",
    "            ac_val.append(val_ac)\n",
    "            \n",
    "    return parameters,index,train_cost_list,val_cost_list,ac_train,ac_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_h = 3 # hidden layer size\n",
    "learning_rate=0.01\n",
    "\n",
    "W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "b1 = np.zeros(shape=(n_h, 1))\n",
    "\n",
    "W2 = np.random.randn(n_h,n_h) * 0.01\n",
    "b2 = np.zeros(shape=(n_h, 1))\n",
    "\n",
    "W3 = np.random.randn(n_y,n_h) * 0.01\n",
    "b3 = np.zeros(shape=(n_y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Training loss : 0.693136\tValidation loss : 0.172780\tTraining Accuracy: 51.703992%\tValidation Accuracy: 50.000000%\n",
      "#300 Training loss : 0.692865\tValidation loss : 0.172804\tTraining Accuracy: 51.314508%\tValidation Accuracy: 50.000000%\n",
      "#600 Training loss : 0.692799\tValidation loss : 0.172831\tTraining Accuracy: 51.314508%\tValidation Accuracy: 50.000000%\n",
      "#900 Training loss : 0.692778\tValidation loss : 0.172847\tTraining Accuracy: 51.314508%\tValidation Accuracy: 50.000000%\n",
      "#1200 Training loss : 0.692764\tValidation loss : 0.172854\tTraining Accuracy: 51.314508%\tValidation Accuracy: 50.000000%\n",
      "#1500 Training loss : 0.692748\tValidation loss : 0.172856\tTraining Accuracy: 51.314508%\tValidation Accuracy: 50.000000%\n",
      "#1800 Training loss : 0.692724\tValidation loss : 0.172853\tTraining Accuracy: 51.314508%\tValidation Accuracy: 50.000000%\n",
      "#2100 Training loss : 0.692684\tValidation loss : 0.172844\tTraining Accuracy: 51.314508%\tValidation Accuracy: 50.000000%\n",
      "#2400 Training loss : 0.692609\tValidation loss : 0.172821\tTraining Accuracy: 51.314508%\tValidation Accuracy: 50.000000%\n",
      "#2700 Training loss : 0.692446\tValidation loss : 0.172762\tTraining Accuracy: 51.314508%\tValidation Accuracy: 50.000000%\n",
      "#3000 Training loss : 0.692024\tValidation loss : 0.172595\tTraining Accuracy: 51.314508%\tValidation Accuracy: 50.000000%\n",
      "#3300 Training loss : 0.690670\tValidation loss : 0.172043\tTraining Accuracy: 51.314508%\tValidation Accuracy: 50.000000%\n",
      "#3600 Training loss : 0.685046\tValidation loss : 0.169802\tTraining Accuracy: 51.606621%\tValidation Accuracy: 52.343750%\n",
      "#3900 Training loss : 0.657489\tValidation loss : 0.159825\tTraining Accuracy: 74.975657%\tValidation Accuracy: 87.109375%\n",
      "#4200 Training loss : 0.562386\tValidation loss : 0.132344\tTraining Accuracy: 82.862707%\tValidation Accuracy: 87.109375%\n",
      "#4500 Training loss : 0.461995\tValidation loss : 0.103438\tTraining Accuracy: 83.836417%\tValidation Accuracy: 88.281250%\n",
      "#4800 Training loss : 0.384249\tValidation loss : 0.091291\tTraining Accuracy: 86.660175%\tValidation Accuracy: 87.890625%\n",
      "#5100 Training loss : 0.313224\tValidation loss : 0.083625\tTraining Accuracy: 89.581305%\tValidation Accuracy: 89.062500%\n",
      "#5400 Training loss : 0.265401\tValidation loss : 0.085557\tTraining Accuracy: 90.749757%\tValidation Accuracy: 88.671875%\n",
      "#5700 Training loss : 0.192368\tValidation loss : 0.099792\tTraining Accuracy: 94.644596%\tValidation Accuracy: 82.812500%\n",
      "#6000 Training loss : 0.263960\tValidation loss : 0.144262\tTraining Accuracy: 88.120740%\tValidation Accuracy: 74.609375%\n",
      "#6300 Training loss : 0.123554\tValidation loss : 0.117795\tTraining Accuracy: 96.397274%\tValidation Accuracy: 81.250000%\n",
      "#6600 Training loss : 0.099897\tValidation loss : 0.133399\tTraining Accuracy: 97.370983%\tValidation Accuracy: 77.343750%\n",
      "#6900 Training loss : 0.208662\tValidation loss : 0.094861\tTraining Accuracy: 92.307692%\tValidation Accuracy: 83.593750%\n",
      "#7200 Training loss : 0.071029\tValidation loss : 0.150781\tTraining Accuracy: 97.857838%\tValidation Accuracy: 78.125000%\n",
      "#7500 Training loss : 0.053367\tValidation loss : 0.173044\tTraining Accuracy: 99.026290%\tValidation Accuracy: 76.171875%\n",
      "#7800 Training loss : 0.043398\tValidation loss : 0.182715\tTraining Accuracy: 99.221032%\tValidation Accuracy: 76.562500%\n",
      "#8100 Training loss : 0.034702\tValidation loss : 0.185863\tTraining Accuracy: 99.610516%\tValidation Accuracy: 76.562500%\n",
      "#8400 Training loss : 0.028833\tValidation loss : 0.192471\tTraining Accuracy: 99.707887%\tValidation Accuracy: 77.343750%\n",
      "#8700 Training loss : 0.023720\tValidation loss : 0.198721\tTraining Accuracy: 99.805258%\tValidation Accuracy: 77.343750%\n",
      "#9000 Training loss : 0.019864\tValidation loss : 0.204675\tTraining Accuracy: 99.902629%\tValidation Accuracy: 77.343750%\n",
      "#9300 Training loss : 0.017561\tValidation loss : 0.210156\tTraining Accuracy: 99.902629%\tValidation Accuracy: 77.734375%\n"
     ]
    }
   ],
   "source": [
    "index = []\n",
    "train_cost_list = []\n",
    "val_cost_list = []\n",
    "ac_train=[]\n",
    "ac_val=[]\n",
    "parameters,index,train_cost_list,val_cost_list,ac_train,ac_val = nn_model(x_train, y_train ,n_h, 10000 , x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.plot(index,train_cost_list)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Loss of Training Set\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(index,val_cost_list)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Loss of Validation Set\")\n",
    "    plt.show()\n",
    "                          \n",
    "    plt.plot(index,ac_train)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Accuracy of Training Set\")\n",
    "    plt.show()\n",
    "                            \n",
    "    plt.plot(index,ac_val)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Accuracy of Validation Set\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headerColor = 'grey'\n",
    "rowEvenColor = 'lightgrey'\n",
    "rowOddColor = 'white'\n",
    "\n",
    "fig = go.Figure(data=[go.Table(\n",
    "header=dict(\n",
    "values=['<b>dataset</b>','<b>loss</b>','<b>accuracy</b>'],\n",
    "line_color='darkslategray',\n",
    "fill_color=headerColor,\n",
    "align=['left','center'],\n",
    "font=dict(color='white', size=12)),\n",
    "cells=dict(\n",
    "values=[\n",
    "    ['training', 'validation'],\n",
    "    [\"{0:.6f}\".format(train_cost_list[-1]), \"{0:.6f}\".format(val_cost_list[-1])],\n",
    "    [\"{0:.6f}\".format(ac_train[-1]), \"{0:.6f}\".format(ac_val[-1])]],\n",
    "    line_color='darkslategray',\n",
    "    # 2-D list of colors for alternating rows\n",
    "#align = ['left', 'center'],\n",
    "font = dict(color = 'darkslategray', size = 11)))])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
